
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

## Case Study: How does a Bike-Share Navigate Speedy Success?

The analysis included in this log has been performed as part of my capstone project for Google's professional certificate in data analytics.

This log will walk through the steps of the main data analysis framework referenced throughout the course:

**Ask**, **Prepare**, **Process**, **Analyze**, **Share**, **Act**

***

### Ask

#### Our Case Prompt

The case study involves acting as a junior analyst on the marketing team at Cyclistic, a bike-sharing company in Chicago. 

Cyclistic's director of marketing believes that the company's future success depends on maximizing the number of annual memberships, members being those who subscribe to the service. Casual riders are those that use the service without a membership.

With the goal of increasing membership in mind, our team wants to analyze Cyclistic's available data to determine how casual riders and annual members differ in their use of the service. From these insights, our team will design a new marketing strategy to convert casual riders into annual members.

This .Rmd file will cover the extent of the analysis - from the start with our initial thoughts and assumptions, cleaning operations, reasoning about analysis and visualization... to our recommendations on strategy. 

The case study prompt is contained within a locked .pdf file, but a link can be found to it within a course site that may be available to you [here](https://www.coursera.org/learn/google-data-analytics-capstone/supplement/7PGIT/case-study-1-how-does-a-bike-share-navigate-speedy-success). 

The link should be offered to you there, but if not then it's because coursera's navigation is a little different depending on whether or not you have created an account. It can be found regardless of account status in the course content for the data analytics capstone project - Week 2.

The report to be hypothetically delivered to Cyclistic executives exists in another file titled "CyclisticReport.Rmd". This one has a lot of extra commentary.

The "ask" phase of the data analysis process does come first, but asking questions should be an iterative process that continues throughout the entirety of any analysis.

Our first questions can be thought of as guiding themes that will be expanded upon or narrowed as we start to find answers to them. Questions like:

1. What data can we use?
    + where is it?
    + who created or collected it?
    + is the data of good quality, and if not can we make it better?
    + what type of information does the data contain?
2. How can we extract useful insight from the data? Or more specifically - how can the data tell us about how different types of users interact with the service?

The end goal of this analysis is to design a marketing strategy aiming to convert casual users to annual members, so we can anticipate further questions surrounding the next theme:

3. How can we use the insight from question (2) to inform this new marketing strategy?

These three themes should be central to all further questions during our analysis, though we will definitely be met with more questions and "asking" throughout its entirety. Asking questions is sort of the core of analysis. Some examples as a preview:

* can we model user behavior when rides aren't recorded with any relationship to the rider?
    + A: kind of, but it would be much more effective if we could identify users from the data. e.g. to build a model that predicts the likelihood of a casual user becoming a member, based on the data collected about users who go on to either become members or cease their use of the service.
* what is important in any given context?
    + A: that is highly contextually dependent and must be constantly reevaluated.
* How do I customize axis labels?
    + A: also contextually dependent, but most of our plots are built with the popular ggplot2 package and so we customize axis labels with the parameters "name=", "breaks=", "labels=", and "limits=" within a call to scale_x or scale_y, with additional identifiers depending on the type of scale you are using - be that discrete, continuous, or otherwise.

I won't go on because you get the point, analysis involves asking a lot of questions and we can't list them all here.

We need to take a look at the data to get some context for our future questions.

***

### Prepare  

We first find the data for the analysis [here](https://divvy-tripdata.s3.amazonaws.com/index.html), where zipped folders contain ride data by the month for Cyclistic. We download the last 12 months of data to get started. The license for the data is located [here](https://www.divvybikes.com/data-license-agreement), where it also explains ownership of the data.

My interpretation of section 2.c of the license suggests that I probably shouldn't aggregate the data and publish it anywhere without the accompanying analysis. 

The reader will regrettably have to download these from the source if they wish to run this .Rmd file. Just ensure that the working directory includes the csv files. For an example see the importing code chunk in this file. I've placed all of the csv files inside one folder in the working directory titled "csvfolder". If you do the same, you won't have to change any code.

I consider myself familiar with spreadsheet software, and have been impressed by many of the powerful things they can do while remaining easy to use. They are incredibly user friendly actually, but a worksheet in Excel holds 1,048,576 rows and Sheets boasts a limit of 5 million. 

We have over 5 million observations to analyze, so it will be more convenient for us to go with another solution.

There are solutions for this within Excel using its data model and power query, and I admit that I only learned how much data we have here by opening it in Excel... but we'll use something more appropriate like a query or programming language. 

We could put them inside a database to use SQL, but R is very capable. Since the data exists in multiple .csv files and not on a database server already, we can use R for everything we need.

The preparing process for a project like this typically includes the ability to participate in dialogue with both the collectors of the data and the stakeholders in the outcome of our analysis. Being a case study, we are limited to only the provided information. Ideally we would be able to ask clarifying questions on process or direction, but this log will inevitably include assumptions due to the impossibility of a dialogue.

We start by loading the tidyverse package. The tidyverse is, as its creators would put it, "an opinionated collection of R packages designed for data science". It's quite popular and widely used, and its functions are incredibly helpful throughout many phases of the data analysis process.

```{r loading tidyverse}
library(tidyverse)
```

Now that we've loaded some helpful tools, we can create a data frame with all of our data using the following code chunk. This raw data we call dat_csv_raw.

The section related to column types coerces our station ID columns into character data. If we don't do this, purrr's map_dfr() function will throw errors about not being able to merge characters and integers. The ID column data exists inconsistently as either datatype, so we simply make them characters for now and we'll decide on how to treat them later.

```{r importing}
mydir <- "csvfolder"
myfiles <- list.files(path=mydir,pattern="*.csv", full.names=TRUE)

dat_csv_raw <- map_dfr(myfiles, read_csv, col_types = cols("start_station_id" = col_character(),"end_station_id" = col_character()))
```

We can now view all of the ride data in one data frame, and see what kind of information we are working with.

```{r head raw data}
head(dat_csv_raw)
```

The data includes information on individual trips or "rides" in wide form. Data like:

* start time
* end time
* start coordinates
* end coordinates
* "ride ID"s
* "station ID"s
* the type of bike used for the trip
* whether the rider was an annual member or casual user

We perform some quick visual sorting and filtering via the GUI in RStudio, and we can immediately spot some potential issues with the ride data.

Notably, as we briefly mentioned in the first example question within the "Ask" section, we are missing any way to connect a ride to an individual. This would be incredibly valuable for analysis, because with it we would be able to more closely examine the specifics surrounding a user's decision to become a member. We could build models to analyze factors that make a user more likely to become a member, and we could aggregate common patterns to build profiles of different user types to further ideate about what factors contribute strongest toward individuals becoming members.

User ID information would be so valuable that I assume the real company that sources the data does actually collect and analyze it, simply because it would be a huge missed opportunity to not do so. The case prompt does mention that the data has been scrubbed of any potentially personally identifying information. So payment information would definitely not be published with the case study, and user identification methods may have been removed based on similar philosophies. In the interest of maintaining the same ideas, I will not mention the real name of the company in this analysis or in my report. I will also refrain from contacting them to ask about their data collection methods.

We will summarize the controllable issues below before going into detail on how to address each:

1. There is a considerable amount of missing data from the station_name columns, as well as additional NA data in other places. We'll need to think about how much of the data is potentially compromised by its missing parts. The station_id and station_name columns also seem to include inconsistent convention in the naming/organization of the stations. This will have to be considered when analyzing anything having to do with the stations.  
2. The coordinate data seems to be rounded to 2 decimal places for a lot of the observations. This makes it harder to pull valuable insight from these points, as coordinates with only two decimal points may not be sensitive enough to communicate a sufficiently precise geographical location.  
3. There are rides included in the data that seem to be from internal testing and not customer info. These rides are quite short, both starting and finishing in a station named "WATSON TESTING - DIVVY".
4. There seem to be some observations where the end time exists before the start time. These will have to be counted and analyzed for their validity, possibly raising questions on the collection methods for the time data.
    + Some observations also seem to suggest that the bike trip was instantaneous, with similar consequences to the above issue.  
5. The case prompt mentions that about eight percent (8%) of riders utilize bikes with accessibility features, though it's not obvious from the data if this information is collected in any meaningful way. From the rideable_type column we can see that there are three types of bikes available to use: classic, electric, and docked bikes. It would help Cyclistic understand how to better serve these users, as well as all users, if they collected data on the accessibility features used by the riders. There could be legal boundaries to collecting this kind of data, but this is definitely something that can spark a valuable conversation with our data collection team.

***

### Process

#### 1 -- NA Data

The existence of NA data in our data frame may place limits on the type of analysis that can be done. We can perform a count of the NAs in each column to get an idea of how compromising this problem actually is. We run:

```{r counting NAs}
sapply(dat_csv_raw,function(x)sum(is.na(x)))
```

We can also use a tidyverse function skimr::skim_without_charts() to get a quick but effective overview of the data:

```{r skim the raw data}
skimr::skim_without_charts(dat_csv_raw)
```

We see from the results of the code chunks that our initial observations were mostly correct regarding which columns were missing data, though there is also some missing data from the ride end coordinates: 4821 rows. This missing data won't affect analysis on ride times, rideable type, or member status, which is good because those are all critical for the scope of this analysis.

We also notice that the number of unique ride IDs is slightly lower (~250) than the total number of rides. This may be an error with data collection, and we'll include a distinct() call when we start cleaning the data in order to ensure that all rows are unique.

Caution should however be applied if any analysis is done on the stations themselves. The missing data in those related columns ranges from 523,781 rows to 567,501 rows. These numbers amount to around ten percent (10%) of the total observations, and can significantly weaken any attempt to analyze the most commonly visited stations.

For example, if we count the number of rides that start at each station and take a look with:

```{r station count}
station_count <- table(dat_csv_raw$start_station_name)
View(station_count)
```

Sorting in descending order we see that it appears most common for a ride to start at the Streeter Dr & Grand Ave station, but it only has a count of 77,587 rides starting there. The missing data on >500,000 station names is more than enough to completely invalidate anything like a "top ten" list of stations.

It's worth asking about these entries, and opening a conversation on how the station name data is collected, but imagine for example that a bug involving a handful of stations invalidated their station name entries. It's possible that six stations exist with more rides than the Streeter Dr & Grand Ave station, but we wouldn't be able to tell from the data.

There are methods for filling in this missing data, but every data set is unique and the inconsistencies in this particular set make it questionably worthwhile. Especially when we consider that the station names themselves are not critical for our goal of determining how casual users and Cyclistic members differ in their use of the service. 

We will speak about two possible methods for rectifying these observations.

First it's very common to solve issues like this by using tidyverse's group_by() and fill() functions, with which we could group the data by either station name or ID and then use fill to easily replace the corresponding missing values. Fill() will simply get the appropriate value from another grouped row that has the correct data.

However, the majority of our rows with missing data include NA values for the station name and the corresponding station ID, so this wouldn't work because we'd have a massive group of NA values from different stations grouped together. This method would fill our data frame with incorrect information.

This method could also work if we grouped our data frame by the coordinate position data, but inconsistencies in the coordinate data will invalidate this as a possibility. We can see in our data frame when we sort by any coordinate column that the values are slightly different, even within the same station. Most stations are listed along a range of coordinate values, differing from each other by ~0.00001. This seems to suggest that the coordinate data may be recorded by a GPS unit on the bike itself, and perhaps a situation where various users leave the bikes at any given location at or near a station. The entries without station names or IDs may be the result of normal use of the service, where a user starts and/or ends their ride without ever visiting a station.

The rounding we have spoken about briefly will also significantly affect our ability to group_by and fill. There is the possibility that two completely different stations have had their coordinate data rounded to the same value, and this type of occurrence would create incorrect data that would be impossible to locate or fix after filling.  

Secondly, these NA values could be filled by applying a lookup function; either inside a for-loop or by using tidyverse's apply capabilities. In theory, this would look through the data frame and would replace the NA values with their appropriate values - with the lookup function able to find those appropriate values by matching station names and IDs according to other entries in the data where they exist correctly.

It seems though, that many stations in this data set exist in multiple rows with different station IDs. This is a significant barrier to using any type of lookup technique, and likely represents a weakness in the data collection methods that requires rectifying. That or potentially just a change in station ID at one point in the past without updating prior records.

We could construct a lookup data frame of rows containing unique station names matched with the most common station ID to appear with them in the data set, but at this point we have no reason to believe that the most common ID for any given name in the set is actually the correct one. We could use the most recent ID if we knew that there had been internal changes, but we'd want to fully understand the data collection before acting certain.

I believe we could clean these names to a satisfactory degree by performing a combination of the above methods. We could first write a function to identify every location within the data where more than one station name is associate with one coordinate pair, and put them all in a table or data frame to peak at for more context. Another data frame with all of the unique coordinates and their corresponding station names could be our lookup key to replacing most or all of the unknown data. There would still be the problem of stations having multiple associated IDs, but IDs are generally useful for internally organizing data anyway - the station names are more important when communicating relevant information about these stations.  

Since we can consider this information to be outside the scope for this project, I'll save this for later. This data set seems valuable for testing certain cleaning methods. For now we'll leave this as an optional exercise for the reader.

With the information that we have, it is my opinion that any concrete analysis of the station names may necessitate a conversation with the data collection team. A conversation about their methodology, as well as determination of our ability to verify a correct list of all station names and their valid station IDs. For our analysis regarding location, we will limit ourselves to the use of coordinates and completely ignore the station names and IDs. In fact we'll remove them in the next code chunk when we remove the testing data, we can't do it before that step or the information on testing data will be gone.

We'll leave the 4821 rows with missing end coordinate data in for now, though later we may want to remove these rows when analyzing said coordinates.

#### 2 -- Rounded Coordinate Data

We can see from a brief glance that much of the coordinate data is rounded to two decimal places. If we want to gather information from the coordinate data, like for example a density map aiming to convey the areas where Cyclistic's services are most or least used, we may not be able to use this rounded coordinate data. We will speak briefly about why it may not be sensitive enough.

Coordinate data communicates a location on the globe based on two angles, measured in degrees and represented by latitude and longitude.

Latitude lines represent the angle between the coordinate location and the equator, measured from the center of the earth and ranging from -90 degrees at the South Pole to 90 degrees at the North Pole. This means that half of the North-South circumference of the earth, or roughly 20,004 kilometers (~12,450 miles) is represented by a range of only 180 degrees, and each degree covers roughly 110 kilometers (~69 miles).

Longitude lines represent the angle between the coordinate location and the prime meridian, measured from the center of the earth and ranging from -180 degrees westward of the prime to 180 degrees eastward of the prime. Meaning that the earth's equatorial circumference of 40,075 kilometers (~24,901 miles) is represented by a range of 360 degrees, though the circumference becomes smaller as you get closer to the poles. The earth is also notably not a perfect sphere, but that's not important to dive deeply into right now.

What is important to note is that the distances between lines of longitude change depending on your latitudinal location, being widest apart at the equator and gradually approaching zero at the poles. Cyclistic exists in Chicago, with most latitudinal readings measured between 41 and 42. At this latitude, the distance between lines of longitude is roughly 85 kilometers (~53 miles).

All of the above means that rounding to two decimal places removes up to 0.55 km (0.34 miles) of latitudinal sensitivity, and up to 0.425 km (0.26 miles) of longitudinal sensitivity. Each rounded coordinate could be up to 0.7 km (0.43 miles) away from the actual geographical location it's attempting to convey. This actually isn't really bad enough to invalidate the coordinates for density maps, as long as we keep it considered.

#### 3 -- TESTING Observations

The observations including the "WATSON TESTING - DIVVY" station appear to be from internal tests and unrelated to actual customer data. The company name from the site containing the data appears as "Divvy", so we will remove these from our data frame.

This will construct a new working data frame we'll call dat_csv_clean, which will let us make comparisons to our raw data whenever we want. We apply a filter to the data frame, and also remove both our unwanted columns and any duplicate rows with:

```{r removing testing data}
dat_csv_clean <- distinct(filter(dat_csv_raw, is.na(start_station_name) | start_station_name != "WATSON TESTING - DIVVY"))

dat_csv_clean <- dat_csv_clean[, !colnames(dat_csv_clean) %in% c("start_station_name","start_station_id","end_station_name","end_station_id")]
```

The filter we apply asks specifically for all rows where the start_station_name column contains NA values, or where they are not "WATSON TESTING - DIVVY". We need to ask explicitly for the NA values because R behaves quite logically regarding NA values, it will filter them out automatically if you don't ask for them.

To explain it simply, there is no way to know whether an unknown item is equivalent to something else - therefore R cannot be certain of what they are and the filter function will remove NAs if not told to keep them. From R's perspective, it is possible that the unknown values actually are "WATSON TESTING - DIVVY".

We can see that this filter brought our total number of observations from 5,136,261 down to 5,135,481. A relatively small change, but these observations have nothing to do with user data and should be excluded.

#### 4 -- Start and End Time Inconsistencies

We saw from a quick glance that there were some rides where the end time actually occurs before the start time. Not only will these affect our analysis, they are indicative of some sort of problem in the collection of these times. We will count these rides to be able to determine how much of an effect they will make.

We will want to do analysis on the length of rides anyway, so we will start here by creating a column that contains the length of each ride.

```{r ride length}
dat_csv_clean$ride_length <- dat_csv_clean$ended_at - dat_csv_clean$started_at
```
This gives us a column containing the length of each ride in seconds.

We count the number of rides with negative trip times by doing a count on the number of ride lengths less than zero:

```{r counting negative ride lengths}
sum(dat_csv_clean$ride_length < 0)
```

We find that there are actually only 3303 rides where this occurs.

There also appears to be a number of rides lasting exactly zero seconds, which we can count by:

```{r counting zero length rides}
sum(dat_csv_clean$ride_length == 0)
```

Finding that there are 455 such rides. The majority of these include a physical distance traveled, making it impossible for them to have only lasted 0 seconds. They don't appear to be limited to a select few stations, so these rides could be indicative of a larger problem with the data collection methods. Perhaps problems with only some specific bikes or time-recording devices.

So we should bring these entries up to the team as they represent an opportunity to improve the data collection process, but it won't significantly harm our analysis to remove them. That is what we'll do by running:

```{r removing negative ride lengths}
dat_csv_clean <- filter(dat_csv_clean, ride_length > 0)
```

#### 5 -- Accessibility Data

The case study prompt mentions that about eight percent (8%) of Cyclistic's user base makes use of available accessibility options. It's not obvious if this information is contained in the data, as there doesn't seem to be a column specifically meant to record information on this.  

It seems like this information could be contained within the rideable_type column, but when we perform a count on it with:

```{r rideable_type count}
table(dat_csv_clean$rideable_type)
```
... and convert them to proportions with:

```{r rideable_type proportions}
prop.table(table(dat_csv_clean$rideable_type))
```
... we can see that none of these rideable types indicate that they are some kind of accessible bike option. The closest proportion to the stated 8% comes from the docked bike type, which looks to be used in about 13% of all rides. Some quick research on industry terminology seems to suggest that a bike's "docked" status refers to the fact that the bike is rented and returned from/to some type of docking station, and has nothing to do with its accessibility.  

This represents an oversight in Cyclistic's data collection methods that could lead to a bias in decision making, and it would be my recommendation to collect information on the accessibility features used by Cyclistic's user base.  

Being able to analyze how users interact with accessibility features could give the company valuable information on how to build a better relationship with those users. The insights would lead to decisions that make Cyclistic more attractive to users who seek certain accessibility features in their bike sharing service, and not all of these users necessarily require said features.  

User experience designers have been putting to practice more and more over recent years the idea that consideration put towards accessibility leads to a benefit for all users... sometimes in unexpected ways. It benefits a company to think about how they can serve all potential users. Many popular features of widely-used tools or products only made their breakthroughs after honest consideration toward every perspective that interacts with their product.  

#### Still Processing - Transforming the data  

There are some things that we want to analyze but aren't clearly represented in the data. We've already added a column for ride length in seconds, but we'll want to make a few more changes:

1. Add a column containing which day of the week the rides took place
2. Add a similar column for the ride's month
3. Add a column holding only the ride's start time

##### 1 -- Adding a weekday column  

We add a column containing the day of the week of the trip's start with R's built-in weekdays() function.

```{r adding weekday column}
dat_csv_clean$weekday <- weekdays(dat_csv_clean$started_at)
```

##### 2 -- Adding a month column

We similarly add a column for the month of the trip with another built-in function, months().

```{r adding month column}
dat_csv_clean$month <- months(dat_csv_clean$started_at)
```

##### 3 -- Adding a time column

We add a column to contain only the time of the trip by formatting the start time to the common %H:%M:%S.

```{r adding time column}
dat_csv_clean$time <- format(dat_csv_clean$started_at,format = "%H:%M:%S")
```

These columns may be unnecessary because there are packages that easily pull this information from our datetimes, but they will be convenient for creating whatever kind of visualizations we need later. We will be able to easily reference the specific time element we want to use, instead of using a function in our code to get them each time from started_at.

***

### Analyze - Visualizing and Analyzing the data

Now that we've got our data clean, we want to build visualizations that will illustrate how the different member types differ in the use of their bikes. Since our data contains information on timing and location, we can analyze the use of Cyclistic's service with respect to timing and location, graphically separating members and casuals to spot differences.

Some ideas on visualizations that would provide context:

1. A chart showing the current proportion of members among all rides. This will show us our starting point, as this proportion is what our marketing director is looking to increase.
2. Graphs illustrating the difference in choice of bicycle type between members and casual users.
3. A display of ride counts as they occur by month, day, and time. Likely multiple visualizations. These will give us clues on how the behavior of the different users change according to month, day, and time. Patterns may emerge that shed light on the different reasons why users choose to use Cyclistic.
4. A display of ride counts by the length of the ride. Similar to the above point, we may see patterns in this ride length data that give context to the different uses of the service.
5. Two density maps that communicate start and end location for rides, one for each member type. If these density maps are significantly different, they may give important clues about different uses of the service. They could also highlight areas of Chicago that need to be marketed to differently by showing that certain areas are over or under-represented in the data. Those types of patterns would lead to "why?" type questions that could be answered with real-world context, and lead to more accurate marketing in the future. E.g. if the use of the service in any particular but unexpected way occurs around a school or other established community, it could give clues about how those particular users interact with the service. Further communication with that community through surveys or conversations could also shed light on factors that we aren't currently considering.

#### 1 -- Proportion of members

We'll create a pie chart to display the proportion of members with the deceptively counter-intuitive code chunk below. It seems like we must make a bar chart and then turn it into a pie chart if we want to use ggplot.

```{r pie chart}
casual_count <- as.integer(count(filter(dat_csv_clean, member_casual == "casual")))
member_count <- as.integer(count(filter(dat_csv_clean, member_casual == "member")))

df_pi <- data.frame(
  group = c("member", "casual"),
  value = c(member_count, casual_count))

df_pi <- df_pi %>% 
  arrange(desc(group)) %>% 
  mutate(prop=value/sum(df_pi$value)*100) %>% 
  mutate(ypos=cumsum(prop)-0.5*prop)

pie <- ggplot(data=df_pi, aes(x="", y=prop, fill=group))

pie + geom_bar(stat='identity', width=1, color="black") +
  coord_polar("y", start=0) +
  theme_void() + #removes background, grid, and numeric labels
  geom_text(aes(y=ypos, label=paste(group,"\n",round(prop,digits=2),"%"),size=6)) +
  scale_fill_brewer(palette="Set2") +
  labs(title="Rides by User Type") +
  theme(plot.title=element_text(size=14,face="bold.italic", hjust=0.5,vjust=0),
        legend.position="none")
```

#### 2 -- Ride counts by type of bicycle

We create a faceted bar chart to view the counts of rides by both bike type and member type with:

```{r rideable_type counts}
ggplot(dat_csv_clean,aes(x=rideable_type)) +
  geom_bar() +
  scale_x_discrete(name="bicycle types", labels=c("Classic", "Docked", "Electric")) +
  labs(title="count of rides by type of bicycle") +
  facet_wrap(~member_casual)
```

We know from the pie chart showing the proportion of members that it's split (roughly) evenly (~54% member vs ~46% casual rides). So the ride count totals for each plot above are close enough to each other that viewing counts like this can communicate information intuitively.

The usage of electric bicycles looks very similar between the two types of users. Where they differ slightly is in their use of electric bikes, and by a larger margin in the use of classic bikes.

We can represent the proportions of these counts with faceted pie charts.

```{r rideable_type proportions visualization, warning = FALSE}
library(data.table)
## rideable pie - start by creating the counts to fill the pie chart dataframe
#member counts
classic_count_m <- as.integer(count(filter(dat_csv_clean, member_casual == "member" & rideable_type == "classic_bike")))
docked_count_m <- as.integer(count(filter(dat_csv_clean, member_casual == "member" & rideable_type == "docked_bike")))
electric_count_m <- as.integer(count(filter(dat_csv_clean, member_casual == "member" & rideable_type == "electric_bike")))

#casual counts
classic_count_c <- as.integer(count(filter(dat_csv_clean, member_casual == "casual" & rideable_type == "classic_bike")))
docked_count_c <- as.integer(count(filter(dat_csv_clean, member_casual == "casual" & rideable_type == "docked_bike")))
electric_count_c <- as.integer(count(filter(dat_csv_clean, member_casual == "casual" & rideable_type == "electric_bike")))

#dataframe for rideable pie
df_ride_pie <- data.frame(
  group = c("member", "member", "member", "casual", "casual", "casual"),
  type = c("classic", "docked", "electric", "classic", "docked", "electric"),
  value = c(classic_count_m, docked_count_m,electric_count_m,classic_count_c, docked_count_c,electric_count_c)
)

df_ride_pie <- df_ride_pie %>%
  arrange(desc(type)) %>%
  mutate(prop=(ifelse(group=="member",value/member_count*100,value/casual_count*100)))

dt_ride_pie <- data.table(df_ride_pie)

dt_ride_pie[,ypos:=cumsum(prop)-0.5*prop, by=group]

ggplot(data=dt_ride_pie, aes(x="", y=prop, fill=type)) +
  geom_col(position="stack", width=1, color="black") +
  geom_text(aes(label=paste(round(prop,digits=1),"%", sep=""), x=1.1),
            position=position_stack(vjust=0.5)) +
  theme_void() +
  labs(title="Proportions of Bike Type by User Type") +
  theme(plot.title=element_text(size=14,
                                face="bold.italic", 
                                hjust=0.5,
                                margin = margin(10,0,10,0)),
        legend.title=element_blank(),
        legend.position = "top",
        strip.text=element_text(size=10,
                                face="bold",
                                vjust=0)) +
  coord_polar("y", start=0) +
  facet_wrap(~group, strip.position = "bottom")
```

Similar to the previous graph, we can see that both types of users are about as likely to use electric bikes, while members more heavily favor classic bikes and casual users show an increased interest in docked bikes.

#### 3 -- Ride counts by month/day/time

We create a count of rides by month with:

```{r month count}
ggplot(dat_csv_clean, aes(x=month, group_by=member_casual, fill=member_casual)) +
  geom_histogram(position="dodge", bins=12, color="black", stat="count") +
  labs(title="Count of rides by Month") +
  scale_fill_brewer(palette="Set2") +
  scale_x_discrete(name="Month", limits=c("January","February","March","April","May","June","July","August","September","October","November","December"),
                   labels=c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")) +
  scale_y_continuous(name="Rides", breaks=c(100000,200000,300000,400000), labels=c("100K","200K","300K","400K")) +
  theme(legend.position=c(0.15,0.75),
        legend.background = element_rect(fill="transparent"),
        legend.title=element_blank())
```

It appears from the above graph that all users are much more likely to use Cyclistic's services during summer months, and the number of casual rides actually surpasses those of members between June and August. All other months consistently show a higher number of member rides than casual.  

This could be because it's typically more pleasant to ride a bike in warmer temperatures, speaking as someone who has used a bicycle in cold weather.  

Annual members may be those that already know they will likely need to use a bike in the winter, but it could also be the case that people who know they have an annual membership are much more likely to consider cycling as an option for their travel at any given time.

We create a count of rides by their day of the week with:

```{r weekday count}
ggplot(dat_csv_clean, aes(x=weekday, group_by=member_casual, fill=member_casual)) +
  geom_histogram(position="dodge", bins=7, stat="count", color="black") +
  labs(title="Count of rides by day of the week") +
  scale_x_discrete(name="Weekday",
                   limits=c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"),
                   labels=c("Sun","Mon","Tues","Wed","Thurs","Fri","Sat")) +
  scale_y_continuous(name="Rides", breaks=seq(100000,500000,100000), labels=c("100K","200K","300K","400K","500K")) +
  scale_fill_brewer(palette="Set2") +
  theme(legend.position = c(0.5,0.96),
        legend.background = element_rect(fill="transparent"),
        legend.title=element_blank(),
        legend.direction="horizontal")
```

From this, we see that there is a much more steady use of the service from members.  

Casuals are more likely to take advantage of Cyclistic's services on the weekends, while member numbers are highest during the week and don't change that much.  

This could indicate a tendency for members to have routines that include a regular commute. Those living in the area who commute a short distance could make frequent use of an annual membership.  

To get a slightly better picture of the phenomena we've observed so far, we should construct a visualization of rides by their time of day.

```{r time of day count}
ggplot(dat_csv_clean, aes(x=format(started_at, format="%H"), group_by=member_casual, fill=member_casual)) +
  geom_histogram(position="dodge", bins=24, color="black", stat="count") +
  labs(title="Count of rides by hour of the day") +
  scale_fill_brewer(palette="Set2") +
  scale_y_continuous(name="Rides", breaks=c(100000,200000,300000), labels=c("100K","200K","300K")) +
  scale_x_discrete(name="Starting Hour (24hr format)") +
  theme(legend.position=c(0.25,0.75),
        legend.background = element_rect(fill="transparent"),
        legend.title=element_blank())
```

This does conform to our ideas about members being commuters. There are two spikes in use from members: one around 8 am when people typically go to work, and one around 5 pm when people are typically leaving work. 

Casual use has only one spike around 5 pm, but none during the typical times people would go to work.  

#### 4 -- Ride counts by the length of the ride

We can try to quickly get an idea of how users differ with respect to ride length with a scatter plot:

```{r ride length scatter plot}
ggplot(dat_csv_clean, aes(x=started_at, y=ride_length, color=member_casual)) +
  geom_point() +
  labs(x="Ride Start", y="length in seconds", title="ride durations") +
  scale_fill_brewer(palette="Set2")
```

It appears as if there are a lot of questionably long rides. One hour is 3,600 seconds, yet there are many rides lasting up to more than 3,000,000 seconds. These could indicate errors in data collection, or they could represent instances where a rider forgot to return a bike for many days. Three million seconds is approximately 34 days, and if Cyclistic charges for late returns then those users may have faced uncomfortable situations upon returning their bikes.

They could also be the result of intended use of the service, where a casual user actually decided to purchase a month of consecutive day passes. We'd have to develop a better understanding of Cyclistic's normal operation.

We can already see from this plot that it seems more likely for casuals to go on longer rides, while member rides are typically shorter.

I want to get a better idea of the frequencies by constructing a histogram that counts rides, binned by the half hour and displaying the count of rides between half-hour separated ticks.

```{r histogram of ride lengths}
ggplot(dat_csv_clean, aes(x=ride_length/60, group_by=member_casual, fill=member_casual)) +
  geom_histogram(position="dodge", binwidth=30, color="black") +
  labs(title="Rides by length and user type") +
  scale_x_continuous(name="Ride length in minutes",breaks=seq(30,300,30),labels=c("30","60","90","120","150","180","210","240","270","300"),limits=c(0,300)) +
  scale_y_continuous(name="rides",breaks=c(0,500000,1000000,1500000,2000000), labels=c("0","0.5M","1M","1.5M","2M")) +
  scale_fill_brewer(palette="Set2") +
  theme(legend.position=c(0.75,0.75),
        legend.background = element_rect(fill="transparent"),
        legend.title=element_blank())
```

We can see from the histogram that our observations from the previous scatter plot were correct. Members are much more likely to take shorter trips. This lines up with the reasoning that members purchase memberships when they have life situations that include opportunities for frequent bike usage, like commuting to school or work.  

We do cut the graph off at 5 hours for the upper end of our ride lengths. This removes exactly 12339 rides from the graph.

A more intuitive way to visualize ride length may be with a box plot. We can construct a box plot from all rides with:

```{r boxplot all rides}
timeplot_data <- dat_csv_clean[colnames(dat_csv_clean) %in% c("ride_length","member_casual")]

ggplot(timeplot_data, aes(x=member_casual, y=ride_length/60)) +
  geom_boxplot() +
  scale_y_continuous(name="Ride length in minutes") +
  theme(axis.title.x=element_blank()) +
  labs(title = "All rides")
```

This is one of the funniest box plots I have ever seen. The data covers a large range of ride lengths, but is so skewed towards the shorter side that the boxes both appear as lines along the axis. There are also so many outliers in this data that the outliers look like they could be the lines of the upper quartile.

Similar to the scatter plot, we can see that casual users are more likely to go on much longer rides than members.  

To get a better view of the ride lengths, we can construct a box plot with no outliers by using the following code chunk.

```{r boxplot no outliers}
ggplot(timeplot_data, aes(x=member_casual, y=ride_length/60)) +
  geom_boxplot(outlier.shape=NA) +
  scale_y_continuous(name="Ride length in minutes",
                     limits=c(0,60)) +
  theme(axis.title.x=element_blank()) +
  labs(title = "All rides, outliers hidden")
```

This paints a much neater picture of the length of the rides.  

It does so by removing ~250 thousand ride observations (248,131), which will necessarily include many rides that we should consider to be normal uses of the service.  

There is value in bringing up the existence of the outrageously long rides with Cyclistic's data collection team. The seemingly month long rides could be a result of data collection errors or normal use, but they could also be the result of casual users forgetting to return their bikes when done with them. 

These types of situations could lead to losses for Cyclistic... if a casual user fears the cost of returning their bike and simply never does, or the unexpected expense results in them not being able to afford to bring it back. Those bikes become unavailable for other users during the whole period, and the negative experience for both affected users may result in them avoiding Cyclistic in the future.

#### 5 -- Density maps showing ride locations

We use the ggmap package to overlay information on ride density over an actual map of Chicago.  

The following code chunk will generate two side-by-side maps showing the density of start times, one for casuals and one for members. This was incredibly taxing on my computer's memory, and I had to manually increase the memory allocated to R. I leave it in this log as evidence of how the graphs were generated, but I advise against running it unless you know it is okay to do so on your machine. Creating these density maps took up to about 10 minutes, and had me a little scared that my laptop would overheat.

```{r density maps}
##commented out because it demands a significant amount of memory to perform, I am able to manually set my working memory to an acceptable level with the next line (memory.limit(size=56000)). If you have a lot of RAM or you use a cloud computing service then this shouldn't be an issue. Ensure that you have enough memory to do this before doing so.
##memory.limit(size=56000) ##DO NOT RUN this line unless you know you need it and it is safe to do so with your machine
# chic_area = c(left=-87.7, bottom=41.85, right=-87.55, top=41.975)
# 
# ride_map <- get_stamenmap(bbox=chic_area, zoom=14, maptype="toner-lite")
# 
# ride_map <- ggmap(ride_map, extent="device",legend="none")
# 
# ride_map <- ride_map + stat_density2d(data=dat_csv_clean, aes(x=start_lng, y=start_lat, fill=..level..,alpha=..level..), geom="polygon") +
#   scale_fill_gradientn(colors=rev(brewer.pal(7, "Spectral"))) +
#   guides(size="none", alpha= "none") +
#   ggtitle("Trip start density") +
#   theme_bw() +
#   facet_wrap(~member_casual)
# 
# print(ride_map)
```
![](https://i.imgur.com/oXJGauf.jpg)
This image is the result of the commented code.

We can see from the maps that the density of member trips is more evenly distributed than the casual trips, whose rides tend to concentrate around the waterfront tourist attractions. This fits well with the ideas we've constructed so far about members tending to be commuters.

Any type of ad campaign targeting casual users could best be implemented in areas around the waterfront tourist attractions, where casual users are more likely to try the service.

When we look at the maps of ride end times, there is little change. The most observable differences are slight shifts in densities along the waterfront.
![](https://i.imgur.com/ixJwPhP.jpg)
Originally I selected the area of Chicago to include in our density map by finding the maximum and minimum latitude and longitude within our starting data, and using those for the boundaries. This needed to be a little fine-tuned though and bounds were then manually selected in order to make the data meaningfully visible. Showing all of our data in one map makes it almost impossible to determine anything, even on a high resolution monitor.  

See for example a map of all ride start times:
![](https://i.imgur.com/KiGAy6H.png)

***

### Share

A much more concise report containing our findings and recommendations can be found within the file "CyclisticReport.Rmd". 

Cyclistic's data informs us that:

* members and casual users differ slightly in their choice of bicycle type, showing that members have tended to prefer classic bikes
* members have been more likely than casual users to use the service on weekdays
* members have been more likely than casual users to use the service during typical commuting times
* members have been more likely than casual users to use the service during colder months of the year
* member rides have tended to be shorter than those of casual users
* casual users have been more likely than members to use the service around tourist attractions along Chicago's waterfront

All of these contribute to a data story that tells us that members are those who believe that they will be able to make consistent use of Cyclistic's services. 

With this in mind, a marketing strategy aiming to convert casual users to members will be focused on making casual users aware of how they would benefit from a membership. Advertising that focuses on the benefits of membership will be able to convince casual users who try the service and enjoy it, barring those with prohibitive circumstances like tourists who primarily reside in areas not served by Cyclistic.

The case study doesn't mention Cyclistic's pricing structure, but I assume that the costs are such that someone frequently using the service will save themselves money by purchasing a membership. For an example with fabricated figures: if the single-day pass is $20 and a membership costs 100 dollars, members will save themselves money if they use the service more than 5 days in their annual period.

Using these artificial prices from the previous paragraph, an ad near areas of heavy casual use simply stating the reality of the monetary situation will likely entice many casual users to purchase a membership. Something like an eye-catching sign near the bikes featuring a photo of cheerful users and text such as: "Use a bike 6 times in a year? Save money by becoming a member". This will range in effectiveness depending on the real costs of each. For example if the number in the ad were above 365, it would surely dissuade potential members.

If the number is as low as 6, it may even make sense for some tourists to purchase annual memberships for the few weeks they are in the area.

Some casual users may take advantage of Cyclistic's services as a means for conveniently showing their visiting friends around Chicago's beautiful waterfront areas. Offering discounts on single-ride or single-day passes to the friends and family of members may entice some of these casual users to purchase memberships. Potentially increasing their overall engagement with the service, as well as creating a mechanic for increasing overall awareness of the service and leading to even more members.

Along the same monetary lines of reasoning, another powerful incentive can be used to convert casuals to members. It's likely that many users will try the service at least once as a casual before committing to a membership. Offering the opportunity to purchase a membership pro-rated with consideration towards money already spent on a single-ride or day pass will likely convert some casuals to members. 

It would essentially give a risk-free trial of the membership, where users could pay for a pass and then have the option for a limited time afterward to purchase a membership minus the cost of their pass. In addition to providing what will likely be viewed as a positive consumer experience, the endorphins and general mood-elevation that accompany physical exercise could contribute to favorable opinions toward the service that they've just enjoyed. Possibly leading to an increased chance of the user purchasing a membership.

In areas farther away from the waterfront and where use seems to be heavily commute-oriented, an additional approach of promoting commuting via bicycle could see success. Advertising which focuses on bicycling as a healthy and eco-friendly method of commuting could convince casual users in these areas to consider Cyclistic as an option for their commuting needs, if they previously haven't.

Bicycling is also considerably cheaper than commuting via vehicle. According to [move.org](https://www.move.org/average-cost-owning-a-car/), as of 2020 the average annual cost of owning a vehicle amounted to $5,264.58 when analyzing all of the USA. Displaying this number in an ad next to the much lower cost of an annual Cyclistic membership would illustrate the value of a membership.

These commuting-oriented ads could be quite simple, something like "Better for the planet. Better for your wallet. Better for you.", though there are a lot of possibilities as commuting by car is demonstrably worse in all three regards.

***

### Act

With all of above for context, my recommendation would be to create a marketing strategy that emphasizes the benefits of a membership. Focusing on:

1. Promoting the benefits of commuting via bicycle through all of Cyclistic's current advertising channels.
    + Environmental benefits
    + Economic benefits
    + Health benefits
2. Making casual users aware of the money-saving benefits of a membership via any medium available. This could be via signage near stations and digitally via email or SMS if the user provides contact information. As well as optionally:
    + offering an opportunity after the use of a single-ride or single-day pass to purchase a membership at a cost reduced by the cost of their single pass
    + offering discounts on single-ride or day passes to friends/referrals of members
3. Visibly participating in events, or partnering with organizations that seek to help people get jobs or scholarships. Essentially anything that would promote more regular bicycle use by those who currently are aware of the service through their past experience with a casual pass. These could be:
    + participation with, or sponsorship of job fairs
    + involvement with scholastic events like multi-school competitions
    + brand presence on college campuses
    + partnering with schools to offer Cyclistic services to all of their students at a reduced cost that's factored into their tuition, similar to partnerships that a lot of schools have with local public transport options
    + programs dedicated toward offering retraining to people with unfortunate financial situations, like temporarily unemployed or unhoused individuals
    + programs dedicated to providing healthy school lunches to children in the area
  
  This type of endeavor would vary in success depending on a number of factors. Company financials would play a large part in deciding what kind of actions are within reach of our marketing team, and I cannot view any of those from our case prompt. I offer these related ideas as a conversation starter with Cyclistic executives. Further deliberation on the best strategy would require more analyses on data that isn't included in the ride data.
  
  There is potential for these actions in recommendation 3 to face criticism concerning their intentions. Criticism asking whether their primary focus is to benefit the city and its people or to simply create more members and revenue for Cyclistic. If we could find a route that is financially feasible for the company while accomplishing both of these things, it would be a great success for everybody. Something that can make members feel good about every facet of their decision to purchase a membership; knowing that their actions are benefiting the planet, the local community, as well as their bodies and wallets. It could be a significant motivator for individuals who are aware of the service and thinking about purchasing a membership.

Bonus Recommendations:

4. Collect customer ID information to better analyze behavior. Being able to analyze rides with relation to their user would allow the company to perform much more thorough statistical analysis. We could build models that estimate the likelihood of a casual user becoming a member using data from the rides that members took as a casuals. We could also construct ideas surrounding the different types of users who engage with the service in order to better investigate their motivations.
5. Collect data on the use of accessibility options to build better relationships with users looking for those options.

### References and Packages

#### Packages

R Core Team (2013). R: A language and environment for statistical
  computing. R Foundation for Statistical Computing, Vienna, Austria.
  URL [http://www.R-project.org/](http://www.R-project.org/)

tidyverse:
Wickham et al., (2019). Welcome to
  the tidyverse. Journal of Open
  Source Software, 4(43), 1686,
  [https://doi.org/10.21105/joss.01686](https://doi.org/10.21105/joss.01686)
  
ggmap:
D. Kahle and H. Wickham. ggmap:
  Spatial Visualization with ggplot2.
  The R Journal, 5(1), 144-161. URL
  [http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf](http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf)
  
data.table:
Matt Dowle and Arun Srinivasan
  (2021). data.table: Extension of
  `data.frame`. R package version
  1.14.2.
  [https://CRAN.R-project.org/package=data.table](https://CRAN.R-project.org/package=data.table)
  
RColorBrewer:
Erich Neuwirth (2014). RColorBrewer:
  ColorBrewer Palettes. R package
  version 1.1-2.
  [https://CRAN.R-project.org/package=RColorBrewer](https://CRAN.R-project.org/package=RColorBrewer)